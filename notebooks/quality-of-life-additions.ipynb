{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f1fe45",
   "metadata": {},
   "source": [
    "This notebook contains an overview of some quality of life improvements introduced over the last few major releases of scikit-learn. Note that I normally prefer to put all of my imports in the top of the notebook, but for learning purposes I will import as necessary. For now we'll just import pandas as numpy and set a magic seed for random states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de55e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SEED = 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cc8de",
   "metadata": {},
   "source": [
    "## Faster parser in fetch_openml [1.2.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82a54b",
   "metadata": {},
   "source": [
    "[OpenML](https://www.openml.org/) is a website that allows users to upload and share machine learning datasets. I haven't used it much, but scikit-learn has functionality to pull datasets directly. Let's pull a dataset so we have something to explore the rest of the quality of life features with. I decided to pull a recently uploaded dataset named [adult](https://www.openml.org/search?type=data&status=active&id=45068) (ID:45068). This is the listed description:\n",
    "\n",
    "> Prediction task is to determine whether a person makes over 50K a year. Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n",
    "    \n",
    "Pulling data into memory is relatively straightforward. Just use the fetch_openml function from sklearn.datasets. However, be sure to specify the data_id rather than the name alone, as there are likely multiple datasets with similar names (particularly for simple names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "data = fetch_openml(\n",
    "    name=None, # Optional, I would definitely specify data_id\n",
    "    version='active', # It is possible to specify an exact version\n",
    "    data_id=45068,\n",
    "    data_home='../data/', # You can specify the data location here!\n",
    "    target_column='default-target', # Allows you to specify the target column(s)\n",
    "    cache=True,\n",
    "    return_X_y=False, # Possible to return (X,y) instead of data dictionary\n",
    "    as_frame='auto', # Bit misleading, will return a dataframe as an element in the data dictionary\n",
    "    parser='liac-arff' # Your choices are pandas or liac-arff. The latter is a pure-Python method.\n",
    ")\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c737bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data['frame']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165efdd",
   "metadata": {},
   "source": [
    "Based on the description, the last column is the binary target. However, we can see that the fetch_openml function has a version of the dataframe seperated into data and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c78471",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d25d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7371f16f",
   "metadata": {},
   "source": [
    "Incidentally, I found that the built-in parser, liac-arff, does not handle missing values properly. Remember to inspect your data, kids!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1155c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['occupation'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd59dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml(\n",
    "    name=None, # Optional, I would definitely specify data_id\n",
    "    version='active', # It is possible to specify an exact version\n",
    "    data_id=45068,\n",
    "    data_home='../data/', # You can specify the data location here!\n",
    "    target_column='default-target', # Allows you to specify the target column(s)\n",
    "    cache=True,\n",
    "    return_X_y=False, # Possible to return (X,y) instead of data dictionary\n",
    "    as_frame='auto', # Bit misleading, will return a dataframe as an element in the data dictionary\n",
    "    parser='pandas' # Your choices are pandas or liac-arff. The latter is a pure-Python method.\n",
    ")\n",
    "df = data['frame']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['occupation'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a5ea9",
   "metadata": {},
   "source": [
    "## Feature Names Support [1.0.2] \n",
    "## get_feature_names_out Available in all Transformers [1.1.3]\n",
    "## Pandas output with set_output API [1.2.1]\n",
    "\n",
    "Now that we have some data, let's talk about some of our fancy new features. Coming from the pre-1.0 release days, applying transformations and training models directly from dataframes was somewhat fraught. Sometimes the model would fail, and it definitely would not pass through the column names to the transformer/model. There was support in some models for defining and inspecting features (I'm looking at you, [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)), but the poor data science practicioner was forced to set the feature names directly.\n",
    "\n",
    "Let's start by looking at the data. We already know we have categorical features, as well as some missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd05a3",
   "metadata": {},
   "source": [
    "Normally, you would need to explicitly declare the datatype for categorical variables. With fetch_openml, however, it will set the datatype for you. Also, as we saw above, there are several feature columns with missing values. Let's create a transformer pipeline to both encode the categorical variables and impute missing values on the numerical feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5811d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# There are multiple ways to do this, but I am explicitly building a list of features (lazily).\n",
    "categorical_features = list(df.dtypes[df.dtypes == 'category'].index)\n",
    "numerical_features = list(df.dtypes[df.dtypes != 'category'].index)\n",
    "# Drop the target, which is included in the numerical features list.\n",
    "numerical_features.remove('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812d68b",
   "metadata": {},
   "source": [
    "Now we can create our transformation pipelines for each feature type and encapsulate them in a ColumnTransformer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline(\n",
    "    [\n",
    "        ('impute', SimpleImputer(strategy='median')),\n",
    "        ('scale', RobustScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        ('encode', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110e643",
   "metadata": {},
   "source": [
    "Now we'll create a single pipeline for training the model. Technically, we don't need to use a pipeline, but it's a nice mechanism for keeping things neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('prep', prep),\n",
    "        ('model', LogisticRegression(max_iter=2000, fit_intercept=False))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65400a",
   "metadata": {},
   "source": [
    "Finally, we can create a train-test split and train a model. I'll also go ahead and cast the target to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['class'].map({'<=50K':0, '>50K':1}).values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f53057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['class'].map({'<=50K':0, '>50K':1}).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, train_size=0.7, shuffle=True, random_state=SEED, stratify=y)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c08cf",
   "metadata": {},
   "source": [
    "This rich view is actually somewhat new to me! It was actually released in [0.23.2](https://scikit-learn.org/0.23/auto_examples/release_highlights/plot_release_highlights_0_23_0.html) and offers a nice way to explore complicated pipelines. You can click the html figure to expand elements and view the params.\n",
    "\n",
    "Now let's use the get_feature_names_out method to retrieve the feature names and pair them with the coefficients to inspect the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe['prep'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coef = pd.DataFrame(\n",
    "    {\n",
    "        'feature':pipe['prep'].get_feature_names_out(),\n",
    "        'coef':pipe['model'].coef_[0]\n",
    "    }\n",
    ")\n",
    "df_coef = df_coef.sort_values('coef', ascending=False)\n",
    "df_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coef.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80474d",
   "metadata": {},
   "source": [
    "Unfortunately, there are some limitations. If we try to introduce PolynomialFeatures transformer _after_ the ColumnTransformer transformer, we lose the traceback to feature names. We also can't incorporate it into the ColumnTransformer, but you can explicitly set the input features for the downstream transformer by assigning the output feature names from the ColumnTransformer to the input feature names for the PolynomialFeatures transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "numerical_transformer = Pipeline(\n",
    "    [\n",
    "        ('impute', SimpleImputer(strategy='median')),\n",
    "        ('scale', RobustScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        ('encode', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('prep', prep),\n",
    "        ('poly', PolynomialFeatures()),\n",
    "        ('model', LogisticRegression(max_iter=2000, fit_intercept=False))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c95758",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad095610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coef = pd.DataFrame(\n",
    "    {\n",
    "        'feature':pipe['poly'].get_feature_names_out(),\n",
    "        'coef':pipe['model'].coef_[0]\n",
    "    }\n",
    ")\n",
    "df_coef = df_coef.sort_values('coef', ascending=False)\n",
    "df_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa535d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe['poly'].feature_names_in_ = pipe['prep'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37325541",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe['poly'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c16d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coef = pd.DataFrame(\n",
    "    {\n",
    "        'feature':pipe['poly'].get_feature_names_out(),\n",
    "        'coef':pipe['model'].coef_[0]\n",
    "    }\n",
    ")\n",
    "df_coef = df_coef.sort_values('coef', ascending=False)\n",
    "df_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coef.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24bed26",
   "metadata": {},
   "source": [
    "Alternatively, you can use the new functionality for setting the output of a transformer introduced in in 1.2.1. However, pandas output does not support sparse matrices, so any one hot encoded outputs will need to be returned as dense values. Keep in mind that this will increase memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ee639",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline(\n",
    "    [\n",
    "        ('impute', SimpleImputer(strategy='median')),\n",
    "        ('scale', RobustScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        ('encode', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('prep', prep),\n",
    "        ('poly', PolynomialFeatures()),\n",
    "        ('model', LogisticRegression(max_iter=2000, fit_intercept=False))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b9d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coef = pd.DataFrame(\n",
    "    {\n",
    "        'feature':pipe['poly'].get_feature_names_out(),\n",
    "        'coef':pipe['model'].coef_[0]\n",
    "    }\n",
    ")\n",
    "df_coef = df_coef.sort_values('coef', ascending=False)\n",
    "df_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d015f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coef.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6063461",
   "metadata": {},
   "source": [
    "## Keyword and positional arguments [1.0.2]\n",
    "\n",
    "This is really more of a stylistic choice, but the maintainers of scikit-learn have decided to enforce the usage of keyword arguments. This isn't necessarily a bad thing, but you should be aware that code created prior to this release may break. Also, rather confusingly, some positional arguments will still work.\n",
    "\n",
    "For instance, let's instantiate a LogisticRegression model with keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca71f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keyword = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    dual=False,\n",
    "    tol=0.0001,\n",
    "    C=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad51e5a",
   "metadata": {},
   "source": [
    "Now try the same thing with positional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c01043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_position = LogisticRegression('l1', False, 0.0001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_position = LogisticRegression('l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84b0dd",
   "metadata": {},
   "source": [
    "You can, however, still instantiate a new object w/ one or two positional keywords, depending on the class. Do your future self a favor, however, and just write out the keywords!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c4ace",
   "metadata": {},
   "source": [
    "## New and enhanced displays [1.2.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852645e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn1dot2]",
   "language": "python",
   "name": "conda-env-sklearn1dot2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
